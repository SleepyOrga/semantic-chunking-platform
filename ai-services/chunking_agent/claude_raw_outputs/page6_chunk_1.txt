[
{
"section_title": "Shortcut Connections and Network Architecture",
"content": "of nonlinear layers. The formulation of $\\mathcal{F}(\\mathbf{x}) + \\mathbf{x}$ can be realized by feedforward neural networks with \" shortcut connections \" (Fig. 2 ). Shortcut connections [2, 33, 48] are those skipping one or more layers. In our case, the shortcut connections simply perform identity mapping, and their outputs are added to the outputs of the stacked layers (Fig. 2 ). Identity shortcut connections add neither extra parameter nor computational complexity. The entire network can still be trained end-to-end by SGD with backpropagation, and can be easily implemented using common libraries ( e.g. , Caffe [19] ) without modifying the solvers.",
"tags": ["neural networks", "shortcut connections", "identity mapping", "sgd", "backpropagation", "caffe"]
},
{
"section_title": "Experimental Results Overview",
"content": "We present comprehensive experiments on ImageNet [35] to show the degradation problem and evaluate our method. We show that: 1) Our extremely deep residual nets are easy to optimize, but the counterpart \" plain \" nets (that simply stack layers) exhibit higher training error when the depth increases; 2) Our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks.",
"tags": ["imagenet", "experiments", "residual networks", "optimization", "deep learning", "training error"]
},
{
"section_title": "CIFAR-10 Results",
"content": "Similar phenomena are also shown on the CIFAR-10 set [20] , suggesting that the optimization difficulties and the effects of our method are not just akin to a particular dataset. We present successfully trained models on this dataset with over 100 layers, and explore models with over 1000 layers.",
"tags": ["cifar-10", "deep networks", "100 layers", "1000 layers", "optimization"]
},
{
"section_title": "ImageNet Classification Performance",
"content": "On the ImageNet classification dataset [35] , we obtain excellent results by extremely deep residual nets. Our 152layer residual net is the deepest network ever presented on ImageNet, while still having lower complexity than VGG nets [40] . Our ensemble has 3.57 % top-5 error on the ImageNet test set, and won the 1st place in the ILSVRC 2015 classification competition .",
"tags": ["imagenet", "classification", "152 layers", "vgg", "ilsvrc 2015", "top-5 error"]
},
{
"section_title": "Competition Wins and Generalization",
"content": "The extremely deep representations also have excellent generalization performance on other recognition tasks, and lead us to further win the 1st places on: ImageNet detection , ImageNet localization , COCO detection , and COCO segmentation in ILSVRC & COCO 2015 competitions. This strong evidence shows that the residual learning principle is generic, and we expect that it is applicable in other vision and non-vision problems.",
"tags": ["generalization", "detection", "localization", "coco", "segmentation", "competitions", "residual learning"]
},
{
"section_title": "Related Work Section Header",
"content": "## 2. Related Work",
"tags": ["section header", "related work"]
},
{
"section_title": "Residual Representations in Image Recognition",
"content": "Residual Representations. In image recognition, VLAD [18] is a representation that encodes by the residual vectors with respect to a dictionary, and Fisher Vector [30] can be formulated as a probabilistic version [18] of VLAD. Both of them are powerful shallow representations for image retrieval and classification [4, 47] . For vector quantization, encoding residual vectors [17] is shown to be more effective than encoding original vectors.",
"tags": ["vlad", "fisher vector", "residual vectors", "image retrieval", "vector quantization", "classification"]
},
{
"section_title": "Multigrid Methods and PDEs",
"content": "In low-level vision and computer graphics, for solving Partial Differential Equations (PDEs), the widely used Multigrid method [3] reformulates the system as subproblems at multiple scales, where each subproblem is responsible for the residual solution between a coarser and a finer scale. An alternative to Multigrid is hierarchical basis preconditioning [44, 45] , which relies on variables that represent residual vectors between two scales.",
"tags": ["multigrid", "pdes", "computer graphics", "hierarchical basis", "preconditioning", "multi-scale"]
},
{
"section_title": "Solver Convergence and Optimization",
"content": "It has been shown [3, 44, 45] that these solvers converge much faster than standard solvers that are unaware of the residual nature of the solutions. These methods suggest that a good reformulation or preconditioning can simplify the optimization.",
"tags": ["solver convergence", "optimization", "preconditioning", "reformulation"]
},
{
"section_title": "Shortcut Connections Background",
"content": "Shortcut Connections. Practices and theories that lead to shortcut connections [2, 33, 48] have been studied for a long time. An early practice of training multi-layer perceptrons (MLPs) is to add a linear layer connected from the network input to the output [33, 48] . In [43, 24] , a few intermediate layers are directly connected to auxiliary classifiers for addressing vanishing/exploding gradients.",
"tags": ["shortcut connections", "mlp", "auxiliary classifiers", "vanishing gradients", "exploding gradients"]
},
{
"section_title": "Layer Response Centering Methods",
"content": "The papers of [38, 37, 31, 46] propose methods for centering layer responses, gradients, and propagated errors, implemented by shortcut connections. In [43] , an \" inception \" layer is composed of a shortcut branch and a few deeper branches.",
"tags": ["layer responses", "gradients", "inception", "shortcut branch", "centering methods"]
},
{
"section_title": "Highway Networks Comparison",
"content": "Concurrent with our work, \" highway networks \" [41, 42] present shortcut connections with gating functions [15] . These gates are data-dependent and have parameters, in contrast to our identity shortcuts that are parameter-free. When a gated shortcut is \" closed \" (approaching zero), the layers in highway networks represent non-residual functions.",
"tags": ["highway networks", "gating functions", "data-dependent", "parameter-free", "identity shortcuts"]
},
{
"section_title": "Residual vs Highway Networks",
"content": "On the contrary, our formulation always learns residual functions; our identity shortcuts are never closed, and all information is always passed through, with additional residual functions to be learned. In addition, high- 101 --- way networks have not demonstrated accuracy gains with extremely increased depth (e.g., over 100 layers).",
"tags": ["residual functions", "identity shortcuts", "information flow", "deep networks", "100 layers"]
},
{
"section_title": "Deep Residual Learning Section",
"content": "## 3. Deep Residual Learnin",
"tags": ["section header", "deep residual learning"]
},
{
"section_title": "Residual Learning Subsection",
"content": "### 3.1. Residual Learning",
"tags": ["subsection header", "residual learning"]
},
{
"section_title": "Mathematical Formulation of Residual Learning",
"content": "Let us consider $\\mathcal{H}(\\mathbf{x})$ as an underlying mapping to be fit by a few stacked layers (not necessarily the entire net), with $\\mathbf{x}$ denoting the inputs to the first of these layers. If one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions $^2$ , then it is equivalent to hypothesize that they can asymptotically approximate the residual functions, i.e. , $\\mathcal{H}(\\mathbf{x}) - \\mathbf{x}$ (assuming that the input and output are of the same dimensions).",
"tags": ["mathematical formulation", "residual functions", "nonlinear layers", "asymptotic approximation"]
},
{
"section_title": "Residual Function Definition",
"content": "So rather than expect stacked layers to approximate $\\mathcal{H}(\\mathbf{x})$ , we explicitly let these layers approximate a residual function $\\mathcal{F}(\\mathbf{x}) := \\mathcal{H}(\\mathbf{x}) - \\mathbf{x}$ . The original function thus becomes $\\mathcal{F}(\\mathbf{x}) + \\mathbf{x}$ . Although both forms should be able to asymptotically approximate the desired functions (as hypothesized), the case of learning might be different.",
"tags": ["residual function", "function approximation", "learning", "mathematical definition"]
},
{
"section_title": "Motivation from Degradation Problem",
"content": "This reformulation is motivated by the counterintuitive phenomena about the degradation problem (Fig. 1 , left). As we discussed in the introduction, if the added layers can be constructed as identity mappings, a deeper model should have training error no greater than its shallower counterpart. The degradation problem suggests that the solvers might have difficulties in approximating identity mappings by multiple nonlinear layers.",
"tags": ["degradation problem", "identity mappings", "training error", "deep models", "optimization difficulty"]
},
{
"section_title": "Residual Learning Solution",
"content": "With the residual learning reformulation, if identity mappings are optimal, the solvers may simply drive the weights of the multiple nonlinear layers toward zero to approach identity mappings. In real cases, it is unlikely that identity mappings are optimal, but our reformulation may help to precondition the problem. If the optimal function is closer to an identity mapping than to a zero mapping, it should be easier for the solver to find the",
"tags": ["residual learning", "weight optimization", "identity mappings", "preconditioning", "solver optimization"]
}
]