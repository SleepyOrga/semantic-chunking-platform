[
  {
    "section_title": "Projection Shortcuts Analysis",
    "content": "A indeed have no residual learning. C is marginally better than B, and we attribute this to the extra parameters introduced by many (thirteen) projection shortcuts. But the small differences among A/B/C indicate that projection shortcuts are not essential for addressing the degradation problem. So we do not use option C in the rest of this paper, to reduce memory/time complexity and model sizes.",
    "tags": ["projection shortcuts", "residual learning", "degradation problem", "complexity"]
  },
  {
    "section_title": "Identity Shortcuts Importance",
    "content": "Identity shortcuts are particularly important for not increasing the complexity of the bottleneck architectures that are introduced below.",
    "tags": ["identity shortcuts", "bottleneck architectures", "complexity"]
  },
  {
    "section_title": "Bottleneck Design Introduction",
    "content": "Next we describe our deeper nets for ImageNet. Because of concerns on the training time that we can afford, we modify the building block as a bottleneck design. For each residual function F, we use a stack of 3 layers instead of 2 (Fig. 5). The three layers are 1×1, 3×3, and 1×1 convolutions, where the 1×1 layers are responsible for reducing and then increasing (restoring) dimensions, leaving the 3×3 layer a bottleneck with smaller input/output dimensions.",
    "tags": ["bottleneck design", "imagenet", "convolutions", "residual function"]
  },
  {
    "section_title": "Bottleneck Architecture Efficiency",
    "content": "Fig. 5 shows an example, where both designs have similar time complexity. The parameter-free identity shortcuts are particularly important for the bottleneck architectures. If the identity shortcut in Fig. 5 (right) is replaced with projection, one can show that the time complexity and model size are doubled, as the shortcut is connected to the two high-dimensional ends. So identity shortcuts lead to more efficient models for the bottleneck designs.",
    "tags": ["bottleneck architectures", "identity shortcuts", "time complexity", "model efficiency"]
  },
  {
    "section_title": "50-layer ResNet Construction",
    "content": "50-layer ResNet: We replace each 2-layer block in the 34-layer net with this 3-layer bottleneck block, resulting in a 50-layer ResNet (Table 1). We use option B for increasing dimensions. This model has 3.8 billion FLOPs.",
    "tags": ["50-layer resnet", "bottleneck block", "flops", "model construction"]
  },
  {
    "section_title": "Deeper ResNet Models",
    "content": "101-layer and 152-layer ResNets: We construct 101layer and 152-layer ResNets by using more 3-layer blocks (Table 1). Remarkably, although the depth is significantly increased, the 152-layer ResNet (11.3 billion FLOPs) still has lower complexity than VGG-16/19 nets (15.3/19.6 billion FLOPs).",
    "tags": ["101-layer resnet", "152-layer resnet", "vgg comparison", "computational complexity"]
  },
  {
    "section_title": "Depth Benefits and Accuracy Gains",
    "content": "The 50/101/152-layer ResNets are more accurate than the 34-layer ones by considerable margins (Table 3 and 4). We do not observe the degradation problem and thus enjoy significant accuracy gains from considerably increased depth. The benefits of depth are witnessed for all evaluation metrics (Table 3 and 4).",
    "tags": ["accuracy gains", "depth benefits", "degradation problem", "evaluation metrics"]
  },
  {
    "section_title": "State-of-the-art Comparisons",
    "content": "Comparisons with State-of-the-art Methods. In Table 4 we compare with the previous best single-model results. Our baseline 34-layer ResNets have achieved very competitive accuracy. Our 152-layer ResNet has a single-model top-5 validation error of 4.49%. This single-model result outperforms all previous ensemble results (Table 5).",
    "tags": ["state-of-the-art", "single-model results", "validation error", "performance comparison"]
  },
  {
    "section_title": "Ensemble Results and Competition Win",
    "content": "We combine six models of different depth to form an ensemble (only with two 152-layer ones at the time of submitting). This leads to 3.57% top-5 error on the test set (Table 5). This entry won the 1st place in ILSVRC 2015.",
    "tags": ["ensemble", "ilsvrc 2015", "test error", "competition"]
  },
  {
    "section_title": "CIFAR-10 Dataset Introduction",
    "content": "We conducted more studies on the CIFAR-10 dataset [20], which consists of 50k training images and 10k testing images in 10 classes. We present experiments trained on the training set and evaluated on the test set. Our focus is on the behaviors of extremely deep networks, but not on pushing the state-of-the-art results, so we intentionally use simple architectures as follows.",
    "tags": ["cifar-10", "dataset", "deep networks", "simple architectures"]
  },
  {
    "section_title": "CIFAR-10 Architecture Design",
    "content": "The plain/residual architectures follow the form in Fig. 3 (middle/right). The network inputs are 32×32 images, with the per-pixel mean subtracted. The first layer is 3×3 convolutions. Then we use a stack of 6n layers with 3×3 convolutions on the feature maps of sizes {32, 16, 8} respectively, with 2n layers for each feature map size.",
    "tags": ["architecture design", "32x32 images", "convolutions", "feature maps"]
  },
  {
    "section_title": "Network Configuration Details",
    "content": "The numbers of filters are {16, 32, 64} respectively. The subsampling is performed by convolutions with a stride of 2. The network ends with a global average pooling, a 10-way fully-connected layer, and softmax. There are totally 6n+2 stacked weighted layers.",
    "tags": ["filters", "subsampling", "global average pooling", "softmax"]
  },
  {
    "section_title": "Shortcut Connections Configuration",
    "content": "When shortcut connections are used, they are connected to the pairs of 3×3 layers (totally 3n shortcuts). On this dataset we use identity shortcuts in all cases (i.e., option A), so our residual models have exactly the same depth, width, and number of parameters as the plain counterparts.",
    "tags": ["shortcut connections", "identity shortcuts", "residual models", "model parameters"]
  },
  {
    "section_title": "Training Configuration",
    "content": "We use a weight decay of 0.0001 and momentum of 0.9, and adopt the weight initialization in [12] and BN [16] but with no dropout. These models are trained with a minibatch size of 128 on two GPUs. We start with a learning rate of 0.1, divide it by 10 at 32k and 48k iterations, and terminate training at 64k iterations, which is determined on a 45k/5k train/val split.",
    "tags": ["training configuration", "weight decay", "momentum", "learning rate", "batch normalization"]
  },
  {
    "section_title": "Data Augmentation and Testing",
    "content": "We follow the simple data augmentation in [24] for training: 4 pixels are padded on each side, and a 32×32 crop is randomly sampled from the padded image or its horizontal flip. For testing, we only evaluate the single view of the original 32×32 image.",
    "tags": ["data augmentation", "padding", "random sampling", "testing"]
  },
  {
    "section_title": "Network Depth Comparison",
    "content": "We compare n={3,5,7,9}, leading to 20, 32, 44, and 56-layer networks. Fig. 6 (left) shows the behaviors of the plain nets. The deep plain nets suffer from increased depth, and exhibit higher training error when going deeper.",
    "tags": ["network depth", "plain nets", "training error", "depth comparison"]
  },
  {
    "section_title": "Optimization Difficulty Problem",
    "content": "This phenomenon is similar to that on ImageNet (Fig. 4, left) and on MNIST (see [41]), suggesting that such an optimization difficulty is a fundamental problem. Fig. 6 (middle) shows the behaviors of ResNets. Also similar to the",
    "tags": ["optimization difficulty", "imagenet", "mnist", "fundamental problem", "resnet behavior"]
  }
]