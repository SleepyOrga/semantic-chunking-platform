[
  {
    "section_title": "Training Configuration and Hyperparameters",
    "content": "as the plain counterparts. We use a weight decay of 0.0001 and momentum of 0.9, and adopt the weight initialization in [12] and BN [16] but with no dropout. These models are trained with a minibatch size of 128 on two GPUs. We start with a learning rate of 0.1, divide it by 10 at 32k and 48k iterations, and terminate training at 64k iterations, which is determined on a 45k/5k train/val split.",
    "tags": ["training", "hyperparameters", "configuration", "learning rate", "batch size"]
  },
  {
    "section_title": "Data Augmentation and Testing Protocol",
    "content": "We follow the simple data augmentation in [24] for training: 4 pixels are padded on each side, and a 32 × 32 crop is randomly sampled from the padded image or its horizontal flip. For testing, we only evaluate the single view of the original 32 × 32 image.",
    "tags": ["data augmentation", "testing", "preprocessing", "image processing"]
  },
  {
    "section_title": "Network Architecture Comparison",
    "content": "We compare n={3,5,7,9}, leading to 20, 32, 44, and 56-layer networks. Fig. 6 (left) shows the behaviors of the plain nets. The deep plain nets suffer from increased depth, and exhibit higher training error when going deeper. This phenomenon is similar to that on ImageNet (Fig. 4, left) and on MNIST (see [41]), suggesting that such an optimization difficulty is a fundamental problem.",
    "tags": ["network architecture", "comparison", "plain networks", "optimization difficulty", "depth"]
  },
  {
    "section_title": "ResNet Performance Analysis",
    "content": "Fig. 6 (middle) shows the behaviors of ResNets. Also similar to the ImageNet cases (Fig. 4, right), our ResNets manage to overcome the optimization difficulty and demonstrate accuracy gains when the depth increases.",
    "tags": ["resnet", "performance", "accuracy", "depth", "optimization"]
  },
  {
    "section_title": "110-Layer ResNet Training",
    "content": "We further explore n=18 that leads to a 110-layer ResNet. In this case, we find that the initial learning rate of 0.1 is slightly too large to start converging. So we use 0.01 to warm up the training until the training error is below 80% (about 400 iterations), and then go back to 0.1 and continue training. The rest of the learning schedule is as done previously. This 110-layer network converges well (Fig. 6, middle).",
    "tags": ["110-layer", "resnet", "learning rate", "warm up", "convergence"]
  },
  {
    "section_title": "Parameter Efficiency and Results",
    "content": "It has fewer parameters than other deep and thin networks such as FitNet [34] and Highway [41] (Table 6), yet is among the state-of-the-art results (6.43%, Table 6).",
    "tags": ["parameters", "efficiency", "state-of-the-art", "comparison", "results"]
  },
  {
    "section_title": "Layer Response Analysis Introduction",
    "content": "Analysis of Layer Responses. Fig. 7 shows the standard deviations (std) of the layer responses. The responses are the outputs of each 3 × 3 layer, after BN and before other nonlinearity (ReLU/addition). For ResNets, this analysis reveals the response strength of the residual functions.",
    "tags": ["layer responses", "analysis", "standard deviation", "residual functions", "batch normalization"]
  },
  {
    "section_title": "ResNet vs Plain Network Response Comparison",
    "content": "Fig. 7 shows that ResNets have generally smaller responses than their plain counterparts. These results support our basic motivation (Sec. 3.1) that the residual functions might be generally closer to zero than the non-residual functions.",
    "tags": ["resnet", "plain networks", "responses", "residual functions", "motivation"]
  },
  {
    "section_title": "Depth Impact on Response Magnitude",
    "content": "We also notice that the deeper ResNet has smaller magnitudes of responses, as evidenced by the comparisons among ResNet-20, 56, and 110 in Fig. 7. When there are more layers, an individual layer of ResNets tends to modify the signal less.",
    "tags": ["depth", "response magnitude", "signal modification", "layer behavior", "resnet"]
  },
  {
    "section_title": "1000+ Layer Network Exploration",
    "content": "Exploring Over 1000 layers. We explore an aggressively deep model of over 1000 layers. We set n=200 that leads to a 1202-layer network, which is trained as described above. Our method shows no optimization difficulty, and this 10^3 layer network is able to achieve training error < 0.1% (Fig. 6, right).",
    "tags": ["1000 layers", "1202-layer", "deep model", "optimization", "training error"]
  },
  {
    "section_title": "Overfitting in Ultra-Deep Networks",
    "content": "Its test error is still fairly good (7.93%, Table 6). But there are still open problems on such aggressively deep models. The testing result of this 1202-layer network is worse than that of our 110-layer network, although both have similar training error. We argue that this is because of overfitting. The 1202-layer network may be unnecessarily large (19.4M) for this small dataset.",
    "tags": ["overfitting", "test error", "1202-layer", "110-layer", "model size"]
  },
  {
    "section_title": "Regularization Discussion",
    "content": "Strong regularization such as maxout [9] or dropout [13] is applied to obtain the best results ([9, 25, 24, 34]) on this dataset. In this paper, we use no maxout/dropout and just simply impose regularization via deep and thin architectures by design, without distracting from the focus on the difficulties of optimization. But combining with stronger regularization may improve results, which we will study in the future.",
    "tags": ["regularization", "maxout", "dropout", "architecture design", "optimization"]
  },
  {
    "section_title": "PASCAL VOC Detection Results",
    "content": "training data: 07+12, 07++12; test data: VOC 07 test, VOC 12 test; VGG-16: 73.2, 70.4; ResNet-101: 76.4, 73.8",
    "tags": ["pascal voc", "object detection", "vgg-16", "resnet-101", "map"]
  },
  {
    "section_title": "COCO Detection Results",
    "content": "metric: mAP@.5, mAP@[.5, .95]; VGG-16: 41.5, 21.2; ResNet-101: 48.4, 27.2",
    "tags": ["coco", "object detection", "map", "vgg-16", "resnet-101"]
  },
  {
    "section_title": "Object Detection Generalization",
    "content": "Our method has good generalization performance on other recognition tasks. Table 7 and 8 show the object detection baseline results on PASCAL VOC 2007 and 2012 [5] and COCO [26]. We adopt Faster R-CNN [32] as the detection method. Here we are interested in the improvements of replacing VGG-16 [40] with ResNet-101.",
    "tags": ["generalization", "object detection", "pascal voc", "coco", "faster r-cnn"]
  },
  {
    "section_title": "Detection Implementation and Performance Gains",
    "content": "The detection implementation (see appendix) of using both models is the same, so the gains can only be attributed to better networks. Most remarkably, on the challenging COCO dataset we obtain a 6.0% increase in COCO's standard metric (mAP@[.5, .95]), which is a 28% relative improvement. This gain is solely due to the learned representations.",
    "tags": ["detection implementation", "performance gains", "coco", "map", "learned representations"]
  },
  {
    "section_title": "Competition Results",
    "content": "Based on deep residual nets, we won the 1st places in several tracks in ILSVRC & COCO 2015 competitions: ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. The details are in the appendix.",
    "tags": ["competition", "ilsvrc", "coco", "first place", "detection", "localization", "segmentation"]
  },
  {
    "section_title": "References - Gradient Descent Difficulties",
    "content": "[1] Y. Bengio, P. Simard, and P Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157–166, 1994.",
    "tags": ["references", "gradient descent", "long-term dependencies", "neural networks"]
  },
  {
    "section_title": "References - Pattern Recognition",
    "content": "[2] C. M. Bishop. Neural networks for pattern recognition. Oxford university press, 1995.",
    "tags": ["references", "pattern recognition", "neural networks", "textbook"]
  },
  {
    "section_title": "References - Multigrid Tutorial",
    "content": "[3] W. L. Briggs, S. F. McCormick, et al. A Multigrid Tutorial. Siam,",
    "tags": ["references", "multigrid", "tutorial", "numerical methods"]
  }
]