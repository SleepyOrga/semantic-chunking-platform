[
  {
    "section_title": "VGG-19 Reference and Open Question",
    "content": "VGG-19 (19.6 billion FLOPs). This hypothesis, however, is still an open question. See [28].",
    "tags": ["vgg-19", "hypothesis", "open-question", "flops"]
  },
  {
    "section_title": "Network Architecture Examples",
    "content": "Figure 3. Example network architectures for ImageNet. Left : the VGG-19 model [40] (19.6 billion FLOPs) as a reference. Middle : a plain network with 34 parameter layers (3.6 billion FLOPs). Right : a residual network with 34 parameter layers (3.6 billion FLOPs). The dotted shortcuts increase dimensions. Table I shows more details and other variants.",
    "tags": ["network-architecture", "imagenet", "vgg-19", "plain-network", "residual-network", "shortcuts"]
  },
  {
    "section_title": "Residual Network Design",
    "content": "Residual Network. Based on the above plain network, we insert shortcut connections (Fig. 3 , right) which turn the network into its counterpart residual version. The identity shortcuts (Eqn.( 1 )) can be directly used when the input and output are of the same dimensions (solid line shortcuts in Fig. 3 ). When the dimensions increase (dotted line shortcuts in Fig. 3 ), we consider two options: (A) The shortcut still performs identity mapping, with extra zero entries padded for increasing dimensions. This option introduces no extra parameter; (B) The projection shortcut in Eqn.( 2 ) is used to match dimensions (done by 1 × 1 convolutions). For both options, when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2.",
    "tags": ["residual-network", "shortcut-connections", "identity-mapping", "dimension-matching", "projection-shortcut", "convolution"]
  },
  {
    "section_title": "Implementation Details",
    "content": "Our implementation for ImageNet follows the practice in [21, 40] . The image is resized with its shorter side randomly sampled in [256,480] for scale augmentation [40] . A 224 × 224 crop is randomly sampled from an image or its horizontal flip, with the per-pixel mean subtracted [21] . The standard color augmentation in [21] is used.",
    "tags": ["implementation", "imagenet", "image-preprocessing", "scale-augmentation", "data-augmentation", "cropping"]
  },
  {
    "section_title": "Training Configuration",
    "content": "We adopt batch normalization (BN) [16] right after each convolution and before activation, following [16] . We initialize the weights as in [12] and train all plain/residual nets from scratch. We use SGD with a mini-batch size of 256. The learning rate starts from 0.1 and is divided by 10 when the error plateaus, and the models are trained for up to 60×10^4 iterations. We use a weight decay of 0.0001 and a momentum of 0.9. We do not use dropout [13] , following the practice in [16] .",
    "tags": ["batch-normalization", "weight-initialization", "sgd", "learning-rate", "training-parameters", "dropout"]
  },
  {
    "section_title": "Testing Methodology",
    "content": "In testing, for comparison studies we adopt the standard 10-crop testing [21] . For best results, we adopt the fullyconvolutional form as in [40, 12] , and average the scores at multiple scales (images are resized such that the shorter side is in {224, 256, 384, 480, 640}).",
    "tags": ["testing", "10-crop-testing", "fully-convolutional", "multi-scale", "evaluation"]
  },
  {
    "section_title": "Experiments Section",
    "content": "## 4. Experiments",
    "tags": ["experiments", "section-header"]
  },
  {
    "section_title": "ImageNet Classification Setup",
    "content": "We evaluate our method on the ImageNet 2012 classification dataset [35] that consists of 1000 classes. The models are trained on the 1.28 million training images, and evaluated on the 50k validation images. We also obtain a final result on the 100k test images, reported by the test server. We evaluate both top-1 and top-5 error rates.",
    "tags": ["imagenet-2012", "classification", "dataset", "evaluation-metrics", "top-1-error", "top-5-error"]
  },
  {
    "section_title": "Plain Networks Evaluation",
    "content": "Plain Networks. We first evaluate 18-layer and 34-layer plain nets. The 34-layer plain net is in Fig. 3 (middle). The 18-layer plain net is of a similar form. See Table 1 for detailed architectures.",
    "tags": ["plain-networks", "18-layer", "34-layer", "network-architecture", "evaluation"]
  },
  {
    "section_title": "Plain Networks Results and Degradation Problem",
    "content": "The results in Table 2 show that the deeper 34-layer plain net has higher validation error than the shallower 18-layer plain net. To reveal the reasons, in Fig. 4 (left) we compare their training/validation errors during the training procedure. We have observed the degradation problem - the",
    "tags": ["plain-networks", "validation-error", "degradation-problem", "training-procedure", "deep-networks"]
  },
  {
    "section_title": "Network Architecture Table",
    "content": "Table 1. Architectures for ImageNet. Building blocks are shown in brackets (see also Fig. 5 ), with the numbers of blocks stacked. Downsampling is performed by conv3 _ 1, conv4 _ 1, and conv5 _ 1 with a stride of 2.",
    "tags": ["architecture-table", "building-blocks", "downsampling", "convolution-layers", "stride"]
  },
  {
    "section_title": "Training Results Figure",
    "content": "Figure 4. Training on ImageNet. Thin curves denote training error, and bold curves denote validation error of the center crops. Left: plain networks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to their plain counterparts.",
    "tags": ["training-results", "training-error", "validation-error", "plain-networks", "resnets", "comparison"]
  },
  {
    "section_title": "Error Rate Comparison Table",
    "content": "Table 2. Top-1 error ( % , 10-crop testing) on ImageNet validation. Here the ResNets have no extra parameter compared to their plain counterparts. Fig. 4 shows the training procedures.",
    "tags": ["error-rates", "top-1-error", "10-crop-testing", "imagenet-validation", "resnets", "plain-networks"]
  },
  {
    "section_title": "Optimization Difficulty Analysis",
    "content": "34-layer plain net has higher training error throughout the whole training procedure, even though the solution space of the 18-layer plain network is a subspace of that of the 34-layer one. We argue that this optimization difficulty is unlikely to be caused by vanishing gradients. These plain networks are trained with BN [16] , which ensures forward propagated signals to have non-zero variances. We also verify that the backward propagated gradients exhibit healthy norms with BN. So neither forward nor backward signals vanish. In fact, the 34-layer",
    "tags": ["optimization-difficulty", "training-error", "solution-space", "vanishing-gradients", "batch-normalization", "gradient-analysis"]
  }
]